{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    roc_auc_score, \n",
    "    precision_score,\n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    matthews_corrcoef,\n",
    "    confusion_matrix, \n",
    "    classification_report,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "# Import all 6 models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MACHINE LEARNING MODEL TRAINING - HEART DISEASE PREDICTION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n All libraries imported successfully!\")\n",
    "print(f\"\\n Key Libraries:\")\n",
    "print(f\"   â€¢ scikit-learn\")\n",
    "print(f\"   â€¢ XGBoost\")\n",
    "print(f\"   â€¢ pandas, numpy\")\n",
    "print(f\"   â€¢ matplotlib, seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('../data/heart.csv')\n",
    "\n",
    "print(f\"\\n Dataset loaded successfully!\")\n",
    "print(f\"   â€¢ Shape: {df.shape}\")\n",
    "print(f\"   â€¢ Features: {df.shape[1] - 1}\")\n",
    "print(f\"   â€¢ Instances: {df.shape[0]}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "print(f\"\\n Data split into features and target:\")\n",
    "print(f\"   â€¢ X (features) shape: {X.shape}\")\n",
    "print(f\"   â€¢ y (target) shape: {y.shape}\")\n",
    "\n",
    "print(f\"\\n Target distribution:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Split the data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"\\n Data split completed:\")\n",
    "print(f\"   â€¢ Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   â€¢ Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n Training set target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(f\"\\n Test set target distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Split the data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"\\n Data split completed:\")\n",
    "print(f\"   â€¢ Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   â€¢ Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n Training set target distribution:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(f\"\\n Test set target distribution:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE SCALING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform both sets\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\n Features scaled using StandardScaler\")\n",
    "print(f\"   â€¢ Training data scaled: {X_train_scaled.shape}\")\n",
    "print(f\"   â€¢ Test data scaled: {X_test_scaled.shape}\")\n",
    "\n",
    "# Create directory for saved models\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, 'saved_models/scaler.pkl')\n",
    "print(f\"\\n Scaler saved to: saved_models/scaler.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DEFINING EVALUATION FUNCTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def evaluate_model(name, model, X_test, y_test, y_pred, y_proba=None):\n",
    "    \"\"\"\n",
    "    Calculate all 6 required evaluation metrics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    name : str\n",
    "        Model name\n",
    "    model : object\n",
    "        Trained model\n",
    "    X_test : array\n",
    "        Test features\n",
    "    y_test : array\n",
    "        True labels\n",
    "    y_pred : array\n",
    "        Predicted labels\n",
    "    y_proba : array, optional\n",
    "        Predicted probabilities\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATING: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. Accuracy\n",
    "    metrics['Accuracy'] = accuracy_score(y_test, y_pred)\n",
    "    print(f\"âœ“ Accuracy:  {metrics['Accuracy']:.4f}\")\n",
    "    \n",
    "    # 2. Precision\n",
    "    metrics['Precision'] = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    print(f\"âœ“ Precision: {metrics['Precision']:.4f}\")\n",
    "    \n",
    "    # 3. Recall\n",
    "    metrics['Recall'] = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    print(f\"âœ“ Recall:    {metrics['Recall']:.4f}\")\n",
    "    \n",
    "    # 4. F1 Score\n",
    "    metrics['F1'] = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    print(f\"âœ“ F1 Score:  {metrics['F1']:.4f}\")\n",
    "    \n",
    "    # 5. MCC (Matthews Correlation Coefficient)\n",
    "    metrics['MCC'] = matthews_corrcoef(y_test, y_pred)\n",
    "    print(f\"âœ“ MCC:       {metrics['MCC']:.4f}\")\n",
    "    \n",
    "    # 6. AUC Score\n",
    "    try:\n",
    "        if y_proba is not None:\n",
    "            if len(np.unique(y_test)) == 2:  # Binary classification\n",
    "                metrics['AUC'] = roc_auc_score(y_test, y_proba[:, 1])\n",
    "            else:  # Multi-class\n",
    "                metrics['AUC'] = roc_auc_score(y_test, y_proba, multi_class='ovr', average='weighted')\n",
    "            print(f\"âœ“ AUC:       {metrics['AUC']:.4f}\")\n",
    "        else:\n",
    "            metrics['AUC'] = 'N/A'\n",
    "            print(f\"âœ“ AUC:       N/A (probabilities not available)\")\n",
    "    except Exception as e:\n",
    "        metrics['AUC'] = 'N/A'\n",
    "        print(f\"âœ“ AUC:       N/A (error: {str(e)[:30]}...)\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"\\nðŸ“Š Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"\\n Evaluation function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 1/6: LOGISTIC REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize model\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Train model\n",
    "print(\"\\nðŸ”„ Training Logistic Regression...\")\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "print(\" Training completed!\")\n",
    "\n",
    "# Make predictions\n",
    "lr_pred = lr_model.predict(X_test_scaled)\n",
    "lr_proba = lr_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "lr_metrics = evaluate_model(\n",
    "    'Logistic Regression', \n",
    "    lr_model, \n",
    "    X_test_scaled, \n",
    "    y_test, \n",
    "    lr_pred, \n",
    "    lr_proba\n",
    ")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(lr_model, 'saved_models/logistic_regression.pkl')\n",
    "print(f\"\\n Model saved to: saved_models/logistic_regression.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 2/6: DECISION TREE CLASSIFIER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize model\n",
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "\n",
    "# Train model\n",
    "print(\"\\n Training Decision Tree...\")\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "print(\" Training completed!\")\n",
    "\n",
    "# Make predictions\n",
    "dt_pred = dt_model.predict(X_test_scaled)\n",
    "dt_proba = dt_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "dt_metrics = evaluate_model(\n",
    "    'Decision Tree', \n",
    "    dt_model, \n",
    "    X_test_scaled, \n",
    "    y_test, \n",
    "    dt_pred, \n",
    "    dt_proba\n",
    ")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(dt_model, 'saved_models/decision_tree.pkl')\n",
    "print(f\"\\n Model saved to: saved_models/decision_tree.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 3/6: K-NEAREST NEIGHBORS (KNN)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train model\n",
    "print(\"\\n Training KNN...\")\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "print(\" Training completed!\")\n",
    "\n",
    "# Make predictions\n",
    "knn_pred = knn_model.predict(X_test_scaled)\n",
    "knn_proba = knn_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "knn_metrics = evaluate_model(\n",
    "    'KNN', \n",
    "    knn_model, \n",
    "    X_test_scaled, \n",
    "    y_test, \n",
    "    knn_pred, \n",
    "    knn_proba\n",
    ")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(knn_model, 'saved_models/knn.pkl')\n",
    "print(f\"\\n Model saved to: saved_models/knn.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 4/6: NAIVE BAYES (GAUSSIAN)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize model\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# Train model\n",
    "print(\"\\n Training Gaussian Naive Bayes...\")\n",
    "nb_model.fit(X_train_scaled, y_train)\n",
    "print(\" Training completed!\")\n",
    "\n",
    "# Make predictions\n",
    "nb_pred = nb_model.predict(X_test_scaled)\n",
    "nb_proba = nb_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "nb_metrics = evaluate_model(\n",
    "    'Naive Bayes', \n",
    "    nb_model, \n",
    "    X_test_scaled, \n",
    "    y_test, \n",
    "    nb_pred, \n",
    "    nb_proba\n",
    ")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(nb_model, 'saved_models/naive_bayes.pkl')\n",
    "print(f\"\\n Model saved to: saved_models/naive_bayes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 5/6: RANDOM FOREST (ENSEMBLE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "\n",
    "# Train model\n",
    "print(\"\\n Training Random Forest...\")\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "print(\" Training completed!\")\n",
    "\n",
    "# Make predictions\n",
    "rf_pred = rf_model.predict(X_test_scaled)\n",
    "rf_proba = rf_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "rf_metrics = evaluate_model(\n",
    "    'Random Forest', \n",
    "    rf_model, \n",
    "    X_test_scaled, \n",
    "    y_test, \n",
    "    rf_pred, \n",
    "    rf_proba\n",
    ")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf_model, 'saved_models/random_forest.pkl')\n",
    "print(f\"\\n Model saved to: saved_models/random_forest.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL 6/6: XGBOOST (ENSEMBLE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize model\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False)\n",
    "\n",
    "# Train model\n",
    "print(\"\\n Training XGBoost...\")\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "print(\" Training completed!\")\n",
    "\n",
    "# Make predictions\n",
    "xgb_pred = xgb_model.predict(X_test_scaled)\n",
    "xgb_proba = xgb_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "xgb_metrics = evaluate_model(\n",
    "    'XGBoost', \n",
    "    xgb_model, \n",
    "    X_test_scaled, \n",
    "    y_test, \n",
    "    xgb_pred, \n",
    "    xgb_proba\n",
    ")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(xgb_model, 'saved_models/xgboost.pkl')\n",
    "print(f\"\\n Model saved to: saved_models/xgboost.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Combine all results\n",
    "all_results = {\n",
    "    'Logistic Regression': lr_metrics,\n",
    "    'Decision Tree': dt_metrics,\n",
    "    'KNN': knn_metrics,\n",
    "    'Naive Bayes': nb_metrics,\n",
    "    'Random Forest': rf_metrics,\n",
    "    'XGBoost': xgb_metrics\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "results_df = results_df[['Accuracy', 'AUC', 'Precision', 'Recall', 'F1', 'MCC']]\n",
    "\n",
    "print(\"\\n COMPARISON TABLE:\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('model_comparison.csv')\n",
    "print(f\"\\n Results saved to: model_comparison.csv\")\n",
    "\n",
    "# Display as formatted table\n",
    "display(results_df.style.highlight_max(axis=0, props='background-color: lightgreen; font-weight: bold'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BEST PERFORMING MODELS PER METRIC\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for metric in results_df.columns:\n",
    "    if metric == 'AUC':\n",
    "        # Handle AUC separately in case of 'N/A' values\n",
    "        numeric_auc = results_df[metric].replace('N/A', np.nan).astype(float)\n",
    "        if not numeric_auc.isna().all():\n",
    "            best_model = numeric_auc.idxmax()\n",
    "            best_score = numeric_auc.max()\n",
    "            print(f\"\\n {metric:12s}: {best_model:20s} ({best_score:.4f})\")\n",
    "        else:\n",
    "            print(f\"\\n {metric:12s}: N/A\")\n",
    "    else:\n",
    "        best_model = results_df[metric].idxmax()\n",
    "        best_score = results_df[metric].max()\n",
    "        print(f\" {metric:12s}: {best_model:20s} ({best_score:.4f})\")\n",
    "\n",
    "# Overall best model (based on F1 score)\n",
    "overall_best = results_df['F1'].idxmax()\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\" RECOMMENDED MODEL: {overall_best}\")\n",
    "print(f\"   (Highest F1 Score: {results_df.loc[overall_best, 'F1']:.4f})\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATING COMPARISON VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare data for visualization\n",
    "plot_df = results_df.copy()\n",
    "# Convert AUC to numeric, replacing 'N/A' with 0\n",
    "plot_df['AUC'] = pd.to_numeric(plot_df['AUC'], errors='coerce').fillna(0)\n",
    "\n",
    "# Create comparison plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics = ['Accuracy', 'AUC', 'Precision', 'Recall', 'F1', 'MCC']\n",
    "colors = plt.cm.Set3(range(len(plot_df)))\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Bar plot\n",
    "    plot_df[metric].plot(kind='bar', ax=ax, color=colors, edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    ax.set_title(f'{metric} Comparison', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Model', fontsize=11)\n",
    "    ax.set_ylabel(metric, fontsize=11)\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.3f', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Visualization saved as: model_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING TEST DATA FOR STREAMLIT APP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create test dataset with original (unscaled) features\n",
    "test_data = pd.DataFrame(X_test, columns=X.columns)\n",
    "test_data['target'] = y_test.values\n",
    "\n",
    "# Save to CSV\n",
    "test_data.to_csv('test_data.csv', index=False)\n",
    "\n",
    "print(f\"\\n Test data saved!\")\n",
    "print(f\"   â€¢ File: test_data.csv\")\n",
    "print(f\"   â€¢ Shape: {test_data.shape}\")\n",
    "print(f\"   â€¢ Columns: {list(test_data.columns)}\")\n",
    "print(f\"\\n This file can be uploaded to the Streamlit app for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n ALL 6 MODELS TRAINED SUCCESSFULLY!\")\n",
    "\n",
    "print(\"\\n Files Created:\")\n",
    "print(\"   1. saved_models/scaler.pkl\")\n",
    "print(\"   2. saved_models/logistic_regression.pkl\")\n",
    "print(\"   3. saved_models/decision_tree.pkl\")\n",
    "print(\"   4. saved_models/knn.pkl\")\n",
    "print(\"   5. saved_models/naive_bayes.pkl\")\n",
    "print(\"   6. saved_models/random_forest.pkl\")\n",
    "print(\"   7. saved_models/xgboost.pkl\")\n",
    "print(\"   8. model_comparison.csv\")\n",
    "print(\"   9. model_comparison.png\")\n",
    "print(\"   10. test_data.csv\")\n",
    "\n",
    "print(\"\\n Model Performance Summary:\")\n",
    "print(f\"   â€¢ Best Accuracy: {results_df['Accuracy'].max():.4f}\")\n",
    "print(f\"   â€¢ Best F1 Score: {results_df['F1'].max():.4f}\")\n",
    "print(f\"   â€¢ Best MCC: {results_df['MCC'].max():.4f}\")\n",
    "print(f\"   â€¢ Recommended Model: {overall_best}\")\n",
    "\n",
    "print(\"\\n Next Steps:\")\n",
    "print(\"   1. Create Streamlit app (app.py)\")\n",
    "print(\"   2. Create requirements.txt\")\n",
    "print(\"   3. Update README.md\")\n",
    "print(\"   4. Push to GitHub\")\n",
    "print(\"   5. Deploy on Streamlit Cloud\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" READY FOR DEPLOYMENT!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
